{
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "accelerator": "GPU",
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "sourceId": 7154116,
          "sourceType": "datasetVersion",
          "datasetId": 4131177
        },
        {
          "sourceId": 7156755,
          "sourceType": "datasetVersion",
          "datasetId": 4133163
        },
        {
          "sourceId": 7157593,
          "sourceType": "datasetVersion",
          "datasetId": 4133735
        },
        {
          "sourceId": 7157605,
          "sourceType": "datasetVersion",
          "datasetId": 4133744
        }
      ],
      "dockerImageVersionId": 30616,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.utils.data as Data, Dataset, DataLoader\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torch.autograd import Variable\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import copy\n",
        "import glob\n",
        "import time\n",
        "import pandas as pd\n",
        "from typing import Dict, List, Tuple\n",
        "from PIL import Image\n",
        "import random"
      ],
      "metadata": {
        "id": "F1qddAHQ3hTv",
        "execution": {
          "iopub.status.busy": "2023-12-10T01:39:39.102170Z",
          "iopub.execute_input": "2023-12-10T01:39:39.102458Z",
          "iopub.status.idle": "2023-12-10T01:39:41.265652Z",
          "shell.execute_reply.started": "2023-12-10T01:39:39.102428Z",
          "shell.execute_reply": "2023-12-10T01:39:41.264838Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DOANLOAD_DATASET = True\n",
        "LR = 0.0001\n",
        "BATCH_SIZE=32\n",
        "EPOCH = 30\n",
        "OUTPUT_PATH = '/kaggle/working/'"
      ],
      "metadata": {
        "id": "wHR8AhBT3tel",
        "execution": {
          "iopub.status.busy": "2023-12-10T01:39:41.267026Z",
          "iopub.execute_input": "2023-12-10T01:39:41.267435Z",
          "iopub.status.idle": "2023-12-10T01:39:41.271459Z",
          "shell.execute_reply.started": "2023-12-10T01:39:41.267408Z",
          "shell.execute_reply": "2023-12-10T01:39:41.270665Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# 檢查GPU是否可用\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device('cuda')\n",
        "    print(torch.cuda.get_device_name(0))\n",
        "else:\n",
        "    device = torch.device('cpu')\n",
        "    print('GPU not available, using CPU.')"
      ],
      "metadata": {
        "id": "rDNnzpHeWpC6",
        "outputId": "f69a574c-6ad9-4ede-9021-cd275f40d60a",
        "execution": {
          "iopub.status.busy": "2023-12-10T01:39:41.272494Z",
          "iopub.execute_input": "2023-12-10T01:39:41.272743Z",
          "iopub.status.idle": "2023-12-10T01:39:41.364136Z",
          "shell.execute_reply.started": "2023-12-10T01:39:41.272721Z",
          "shell.execute_reply": "2023-12-10T01:39:41.363266Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Tesla T4\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install dill\n",
        "import dill"
      ],
      "metadata": {
        "id": "8s621vKWTbhE",
        "outputId": "a5ce1d08-895b-480b-c324-b35271d9a478",
        "execution": {
          "iopub.status.busy": "2023-12-10T01:39:41.366069Z",
          "iopub.execute_input": "2023-12-10T01:39:41.366339Z",
          "iopub.status.idle": "2023-12-10T01:39:54.174956Z",
          "shell.execute_reply.started": "2023-12-10T01:39:41.366314Z",
          "shell.execute_reply": "2023-12-10T01:39:54.173924Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Requirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (0.3.7)\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CFG:\n",
        "    DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    NUM_DEVICES = torch.cuda.device_count()\n",
        "    NUM_WORKERS = os.cpu_count()\n",
        "    NUM_CLASSES = 45\n",
        "    EPOCHS = 7\n",
        "    BATCH_SIZE = (\n",
        "        32 if torch.cuda.device_count() <= 2\n",
        "        else (32 * torch.cuda.device_count())\n",
        "    )\n",
        "    LR = 0.001\n",
        "    APPLY_SHUFFLE = True\n",
        "    SEED = 768\n",
        "    HEIGHT = 256\n",
        "    WIDTH = 256\n",
        "    CHANNELS = 3\n",
        "    IMAGE_SIZE = (256, 256, 3)"
      ],
      "metadata": {
        "id": "9T7IeL8IMyUo",
        "execution": {
          "iopub.status.busy": "2023-12-10T01:39:54.176403Z",
          "iopub.execute_input": "2023-12-10T01:39:54.176689Z",
          "iopub.status.idle": "2023-12-10T01:39:54.183734Z",
          "shell.execute_reply.started": "2023-12-10T01:39:54.176662Z",
          "shell.execute_reply": "2023-12-10T01:39:54.182766Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reload the dataset\n",
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "\n",
        "DATASET_PATH = '/kaggle/working'\n",
        "\n",
        "# 加載數據集\n",
        "with open('/kaggle/input/usedata/train_loader.pkl','rb') as f:\n",
        "    train_loader = dill.load(f)\n",
        "with open('/kaggle/input/usedata/val_loader.pkl','rb') as f:\n",
        "    val_loader = dill.load(f)\n",
        "with open('/kaggle/input/usedata/test_loader.pkl','rb') as f:\n",
        "    test_loader = dill.load(f)"
      ],
      "metadata": {
        "id": "7anlEKRMJzgb",
        "execution": {
          "iopub.status.busy": "2023-12-10T01:39:54.194117Z",
          "iopub.execute_input": "2023-12-10T01:39:54.194441Z",
          "iopub.status.idle": "2023-12-10T01:39:54.244588Z",
          "shell.execute_reply.started": "2023-12-10T01:39:54.194416Z",
          "shell.execute_reply": "2023-12-10T01:39:54.243789Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 建立存放結果的資料集\n",
        "import os\n",
        "\n",
        "path = '/kaggle/working/test'\n",
        "if not os.path.isdir(path):\n",
        "    os.mkdir(path)\n",
        "\n",
        "path = '/kaggle/working/train'\n",
        "if not os.path.isdir(path):\n",
        "    os.mkdir(path)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-10T01:39:54.245585Z",
          "iopub.execute_input": "2023-12-10T01:39:54.245856Z",
          "iopub.status.idle": "2023-12-10T01:39:54.251296Z",
          "shell.execute_reply.started": "2023-12-10T01:39:54.245825Z",
          "shell.execute_reply": "2023-12-10T01:39:54.250319Z"
        },
        "trusted": true,
        "id": "6T3k66j6H9Kd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Alexnet"
      ],
      "metadata": {
        "id": "AcTBsa0_H9Kf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AlexNet(nn.Module):\n",
        "\n",
        "    def __init__(self, num_classes=45):\n",
        "        super(AlexNet, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=5),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(64, 192, kernel_size=5, padding=2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        )\n",
        "        self.classifier = nn.Linear(256*8*8, num_classes)\n",
        "        #self.classifier = nn.Linear(256 * (image_size_after_pooling // 4) * (image_size_after_pooling // 4), num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "def alexnet(**kwargs):\n",
        "    model = AlexNet(**kwargs)\n",
        "    return model"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-10T01:39:54.252581Z",
          "iopub.execute_input": "2023-12-10T01:39:54.253327Z",
          "iopub.status.idle": "2023-12-10T01:39:54.264081Z",
          "shell.execute_reply.started": "2023-12-10T01:39:54.253291Z",
          "shell.execute_reply": "2023-12-10T01:39:54.263211Z"
        },
        "trusted": true,
        "id": "nT7oDlvvH9Kf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "torch.cuda.empty_cache()\n",
        "alexNet = alexnet().cuda()\n",
        "alexNet = nn.DataParallel(alexNet)\n",
        "\n",
        "optimizer = torch.optim.Adam(alexNet.parameters(), lr=LR)\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "local_train_data = copy.deepcopy(train_loader)\n",
        "#local_train_data = DataLoader(train_dataloader, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# 容器來儲存每一輪的輸出\n",
        "train_predict = torch.Tensor().to(device)\n",
        "train_label = torch.Tensor().to(device)\n",
        "test_predict = torch.Tensor().to(device)\n",
        "test_label = torch.Tensor().to(device)\n",
        "\n",
        "for epoch in range(EPOCH):\n",
        "    alexNet.train()\n",
        "    for step, (x, y) in enumerate(local_train_data):\n",
        "        #b_x = Variable(x, requires_grad=False)\n",
        "        #b_y = Variable(y, requires_grad=False)\n",
        "        b_x = x.to(device)  # 將輸入數據移動到GPU上\n",
        "        b_y = y.to(device)  # 將標籤移動到GPU上\n",
        "        out = alexNet(b_x)\n",
        "        loss = loss_function(out, b_y)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if step % 100 == 0:\n",
        "            print('Epoch: {} | Step: {} | Loss: {}'.format(epoch + 1, step, loss))\n",
        "\n",
        "\n",
        "local_train_loader = copy.deepcopy(train_loader)\n",
        "local_test_loader = copy.deepcopy(test_loader)\n",
        "\n",
        "# 在評估模式下，模型只需要在主 GPU 上執行\n",
        "alexNet = alexNet.module\n",
        "\n",
        "# 測試集\n",
        "alexNet.eval()  # 切換到評估模式\n",
        "model_path = '/kaggle/working/'\n",
        "torch.save(alexNet.state_dict(), model_path+'alexNet.pth')\n",
        "\n",
        "\n",
        "with torch.no_grad():  # 在評估模式下，不計算梯度\n",
        "    for step, (x, y) in enumerate(local_train_loader):\n",
        "        b_x = x.to(device)\n",
        "        b_y = y.to(device)\n",
        "        out = alexNet(b_x)\n",
        "        # 將該批次的預測結果和標籤添加到對應的容器中\n",
        "        train_predict = torch.cat((train_predict, out), dim=0)\n",
        "        train_label = torch.cat((train_label, b_y), dim=0)\n",
        "\n",
        "with torch.no_grad():  # 在評估模式下，不計算梯度\n",
        "    for step, (x, y) in enumerate(local_test_loader):\n",
        "        b_x = x.to(device)\n",
        "        b_y = y.to(device)\n",
        "        out = alexNet(b_x)\n",
        "        # 將該批次的預測結果和標籤添加到對應的容器中\n",
        "        test_predict = torch.cat((test_predict, out), dim=0)\n",
        "        test_label = torch.cat((test_label, b_y), dim=0)\n",
        "\n",
        "# train_predict、train_label 分別包含了每一輪的模型訓練集預測和標籤\n",
        "# test_predict、test_label 分別包含了模型對測試集的預測和標籤\n",
        "print(\"Shape of train_predict:\", train_predict.shape)\n",
        "print(\"Shape of train_label:\", train_label.shape)\n",
        "print(\"Shape of test_predict:\", test_predict.shape)\n",
        "print(\"Shape of test_label:\", test_label.shape)\n",
        "\n",
        "\n",
        "output_path = OUTPUT_PATH+'train'\n",
        "if not os.path.exists(output_path):\n",
        "    os.mkdir(output_path)\n",
        "temp_dict = {'predictionVectors':train_predict,'labelVectors':train_label}\n",
        "torch.save(temp_dict, os.path.join(output_path, 'alexNet_train_prediction.pt'))\n",
        "\n",
        "\n",
        "output_path = OUTPUT_PATH+'test'\n",
        "if not os.path.exists(output_path):\n",
        "    os.mkdir(output_path)\n",
        "temp_dict = {'predictionVectors':test_predict,'labelVectors':test_label}\n",
        "torch.save(temp_dict, os.path.join(output_path, 'alexNet_test_prediction.pt'))\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-10T01:39:54.267096Z",
          "iopub.execute_input": "2023-12-10T01:39:54.267418Z",
          "iopub.status.idle": "2023-12-10T01:39:54.278477Z",
          "shell.execute_reply.started": "2023-12-10T01:39:54.267390Z",
          "shell.execute_reply": "2023-12-10T01:39:54.277599Z"
        },
        "trusted": true,
        "id": "5tVz8_xVH9Kf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DenseNet"
      ],
      "metadata": {
        "id": "xv4Gm77XH9Kg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "\n",
        "from torch.autograd import Variable\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    def __init__(self, inplanes, expansion=4, growthRate=12, dropRate=0):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        planes = expansion * growthRate\n",
        "        self.bn1 = nn.BatchNorm2d(inplanes)\n",
        "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, growthRate, kernel_size=3,\n",
        "                               padding=1, bias=False)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.dropRate = dropRate\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.bn1(x)\n",
        "        out = self.relu(out)\n",
        "        out = self.conv1(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.conv2(out)\n",
        "        if self.dropRate > 0:\n",
        "            out = F.dropout(out, p=self.dropRate, training=self.training)\n",
        "\n",
        "        out = torch.cat((x, out), 1)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    def __init__(self, inplanes, expansion=1, growthRate=12, dropRate=0):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        planes = expansion * growthRate\n",
        "        self.bn1 = nn.BatchNorm2d(inplanes)\n",
        "        self.conv1 = nn.Conv2d(inplanes, growthRate, kernel_size=3,\n",
        "                               padding=1, bias=False)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.dropRate = dropRate\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.bn1(x)\n",
        "        out = self.relu(out)\n",
        "        out = self.conv1(out)\n",
        "        if self.dropRate > 0:\n",
        "            out = F.dropout(out, p=self.dropRate, training=self.training)\n",
        "\n",
        "        out = torch.cat((x, out), 1)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class Transition(nn.Module):\n",
        "    def __init__(self, inplanes, outplanes):\n",
        "        super(Transition, self).__init__()\n",
        "        self.bn1 = nn.BatchNorm2d(inplanes)\n",
        "        self.conv1 = nn.Conv2d(inplanes, outplanes, kernel_size=1,\n",
        "                               bias=False)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.bn1(x)\n",
        "        out = self.relu(out)\n",
        "        out = self.conv1(out)\n",
        "        out = F.avg_pool2d(out, 2)\n",
        "        return out\n",
        "\n",
        "\n",
        "class DenseNet(nn.Module):\n",
        "\n",
        "    def __init__(self, depth=40, block=Bottleneck,\n",
        "        dropRate=0, num_classes=45, growthRate=12, compressionRate=2):\n",
        "        super(DenseNet, self).__init__()\n",
        "\n",
        "        assert (depth - 4) % 3 == 0, 'depth should be 3n+4'\n",
        "        n = (depth - 4) / 3 if block == BasicBlock else (depth - 4) // 6\n",
        "\n",
        "        self.growthRate = growthRate\n",
        "        self.dropRate = dropRate\n",
        "\n",
        "        # self.inplanes is a global variable used across multiple\n",
        "        # helper functions\n",
        "        self.inplanes = growthRate * 2\n",
        "        self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=3, padding=1,\n",
        "                               bias=False)\n",
        "        self.dense1 = self._make_denseblock(block, n)\n",
        "        self.trans1 = self._make_transition(compressionRate)\n",
        "        self.dense2 = self._make_denseblock(block, n)\n",
        "        self.trans2 = self._make_transition(compressionRate)\n",
        "        self.dense3 = self._make_denseblock(block, n)\n",
        "        self.bn = nn.BatchNorm2d(self.inplanes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.avgpool = nn.AvgPool2d(8)\n",
        "        self.fc = nn.Linear(self.inplanes* 64, num_classes)\n",
        "\n",
        "        # Weight initialization\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                m.weight.data.fill_(1)\n",
        "                m.bias.data.zero_()\n",
        "\n",
        "    def _make_denseblock(self, block, blocks):\n",
        "        layers = []\n",
        "        for i in range(blocks):\n",
        "            # Currently we fix the expansion ratio as the default value\n",
        "            layers.append(block(self.inplanes, growthRate=self.growthRate, dropRate=self.dropRate))\n",
        "            self.inplanes += self.growthRate\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def _make_transition(self, compressionRate):\n",
        "        inplanes = self.inplanes\n",
        "        outplanes = int(math.floor(self.inplanes // compressionRate))\n",
        "        self.inplanes = outplanes\n",
        "        return Transition(inplanes, outplanes)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "\n",
        "        x = self.trans1(self.dense1(x))\n",
        "        x = self.trans2(self.dense2(x))\n",
        "        x = self.dense3(x)\n",
        "        x = self.bn(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "def densenet(**kwargs):\n",
        "    '''\n",
        "    Constructs a ResNet model.\n",
        "    '''\n",
        "    return DenseNet(**kwargs)"
      ],
      "metadata": {
        "id": "_gCEV3pyH9Kg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "Densenet = densenet().cuda()\n",
        "\n",
        "Densenet = nn.DataParallel(Densenet)\n",
        "\n",
        "optimizer = torch.optim.Adam(Densenet.parameters(), lr=LR)\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "local_train_data = copy.deepcopy(train_loader)\n",
        "\n",
        "for epoch in range(EPOCH):\n",
        "    Densenet.train()\n",
        "    for step, (x, y) in enumerate(local_train_data):\n",
        "        # b_x = Variable(x, requires_grad=False)\n",
        "        # b_y = Variable(y, requires_grad=False)\n",
        "        b_x = x.to(device)  # 將輸入數據移動到GPU上\n",
        "        b_y = y.to(device)  # 將標籤移動到GPU上\n",
        "        out = Densenet(b_x)\n",
        "        loss = loss_function(out, b_y)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if step % 100 == 0:\n",
        "            print('Epoch: {} | Step: {} | Loss: {}'.format(epoch + 1, step, loss))\n",
        "\n",
        "# 容器來儲存每一輪的輸出\n",
        "train_predict = torch.Tensor().to(device)\n",
        "train_label = torch.Tensor().to(device)\n",
        "test_predict = torch.Tensor().to(device)\n",
        "test_label = torch.Tensor().to(device)\n",
        "\n",
        "\n",
        "# 在評估模式下，模型只需要在主 GPU 上執行\n",
        "Densenet = Densenet.module\n",
        "\n",
        "local_train_loader = copy.deepcopy(train_loader)\n",
        "local_test_loader = copy.deepcopy(test_loader)\n",
        "# 測試集\n",
        "Densenet.eval()  # 切換到評估模式\n",
        "model_path = '/kaggle/working/'\n",
        "torch.save(Densenet.state_dict(), model_path+'Densenet.pth')\n",
        "\n",
        "\n",
        "with torch.no_grad():  # 在評估模式下，不計算梯度\n",
        "    for step, (x, y) in enumerate(local_train_loader):\n",
        "        b_x = x.to(device)\n",
        "        b_y = y.to(device)\n",
        "        out = Densenet(b_x)\n",
        "        # 將該批次的預測結果和標籤添加到對應的容器中\n",
        "        train_predict = torch.cat((train_predict, out), dim=0)\n",
        "        train_label = torch.cat((train_label, b_y), dim=0)\n",
        "\n",
        "with torch.no_grad():  # 在評估模式下，不計算梯度\n",
        "    for step, (x, y) in enumerate(local_test_loader):\n",
        "        b_x = x.to(device)\n",
        "        b_y = y.to(device)\n",
        "        out = Densenet(b_x)\n",
        "        # 將該批次的預測結果和標籤添加到對應的容器中\n",
        "        test_predict = torch.cat((test_predict, out), dim=0)\n",
        "        test_label = torch.cat((test_label, b_y), dim=0)\n",
        "\n",
        "# train_predict、train_label 分別包含了每一輪的模型訓練集預測和標籤\n",
        "# test_predict、test_label 分別包含了模型對測試集的預測和標籤\n",
        "print(\"Shape of train_predict:\", train_predict.shape)\n",
        "print(\"Shape of train_label:\", train_label.shape)\n",
        "print(\"Shape of test_predict:\", test_predict.shape)\n",
        "print(\"Shape of test_label:\", test_label.shape)\n",
        "\n",
        "\n",
        "output_path = OUTPUT_PATH+'train'\n",
        "if not os.path.exists(output_path):\n",
        "    os.mkdir(output_path)\n",
        "temp_dict = {'predictionVectors':train_predict,'labelVectors':train_label}\n",
        "torch.save(temp_dict, os.path.join(output_path, 'Densenet_train_prediction.pt'))\n",
        "\n",
        "\n",
        "output_path = OUTPUT_PATH+'test'\n",
        "if not os.path.exists(output_path):\n",
        "    os.mkdir(output_path)\n",
        "temp_dict = {'predictionVectors':test_predict,'labelVectors':test_label}\n",
        "torch.save(temp_dict, os.path.join(output_path, 'Densenet_test_prediction.pt'))\n"
      ],
      "metadata": {
        "id": "u5xZMCxeH9Kh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ResNet"
      ],
      "metadata": {
        "id": "J4E9HFYFVOO2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "import copy\n",
        "import os\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "from torch.autograd import Variable"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-10T01:39:54.279510Z",
          "iopub.execute_input": "2023-12-10T01:39:54.280079Z",
          "iopub.status.idle": "2023-12-10T01:39:54.290074Z",
          "shell.execute_reply.started": "2023-12-10T01:39:54.280041Z",
          "shell.execute_reply": "2023-12-10T01:39:54.289270Z"
        },
        "trusted": true,
        "id": "BHF4RyAqH9Ki"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import math\n",
        "\n",
        "def conv3x3(in_planes, out_planes, stride=1):\n",
        "    \"3x3 convolution with padding\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
        "                     padding=1, bias=False)\n",
        "\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = conv3x3(planes, planes)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            residual = self.downsample(x)\n",
        "\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n",
        "                               padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(planes * 4)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn3(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            residual = self.downsample(x)\n",
        "\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "\n",
        "    def __init__(self, depth, num_classes=45, block_name='BasicBlock'):\n",
        "        super(ResNet, self).__init__()\n",
        "        # Model type specifies number of layers for CIFAR-10 model\n",
        "        if block_name.lower() == 'basicblock':\n",
        "            assert (depth - 2) % 6 == 0, 'When use basicblock, depth should be 6n+2, e.g. 20, 32, 44, 56, 110, 1202'\n",
        "            n = (depth - 2) // 6\n",
        "            block = BasicBlock\n",
        "        elif block_name.lower() == 'bottleneck':\n",
        "            assert (depth - 2) % 9 == 0, 'When use bottleneck, depth should be 9n+2, e.g. 20, 29, 47, 56, 110, 1199'\n",
        "            n = (depth - 2) // 9\n",
        "            block = Bottleneck\n",
        "        else:\n",
        "            raise ValueError('block_name shoule be Basicblock or Bottleneck')\n",
        "\n",
        "\n",
        "        self.inplanes = 16\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1,\n",
        "                               bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(16)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.layer1 = self._make_layer(block, 16, n)\n",
        "        self.layer2 = self._make_layer(block, 32, n, stride=2)\n",
        "        self.layer3 = self._make_layer(block, 64, n, stride=2)\n",
        "        self.avgpool = nn.AvgPool2d(8)\n",
        "        self.fc = nn.Linear(64 * block.expansion * 64, num_classes)\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                m.weight.data.fill_(1)\n",
        "                m.bias.data.zero_()\n",
        "\n",
        "    def _make_layer(self, block, planes, blocks, stride=1):\n",
        "        downsample = None\n",
        "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                nn.Conv2d(self.inplanes, planes * block.expansion,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(planes * block.expansion),\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
        "        self.inplanes = planes * block.expansion\n",
        "        for i in range(1, blocks):\n",
        "            layers.append(block(self.inplanes, planes))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)    # 32x32\n",
        "\n",
        "        x = self.layer1(x)  # 32x32\n",
        "        x = self.layer2(x)  # 16x16\n",
        "        x = self.layer3(x)  # 8x8\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "def resnet(**kwargs):\n",
        "    '''\n",
        "    Constructs a ResNet model.\n",
        "    '''\n",
        "    return ResNet(**kwargs)"
      ],
      "metadata": {
        "id": "An53eKng9E4y",
        "execution": {
          "iopub.status.busy": "2023-12-10T01:39:54.291238Z",
          "iopub.execute_input": "2023-12-10T01:39:54.291624Z",
          "iopub.status.idle": "2023-12-10T01:39:54.319619Z",
          "shell.execute_reply.started": "2023-12-10T01:39:54.291598Z",
          "shell.execute_reply": "2023-12-10T01:39:54.318728Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "Resnet_20 = resnet(depth=20).cuda()\n",
        "# 使用 DataParallel 將模型複製到多個 GPU\n",
        "Resnet_20 = nn.DataParallel(Resnet_20)\n",
        "\n",
        "optimizer = torch.optim.Adam(Resnet_20.parameters(), lr=LR)\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "local_train_data = copy.deepcopy(train_loader)\n",
        "\n",
        "for epoch in range(EPOCH):\n",
        "    Resnet_20.train()\n",
        "    for step, (x, y) in enumerate(local_train_data):\n",
        "        # b_x = Variable(x, requires_grad=False)\n",
        "        # b_y = Variable(y, requires_grad=False)\n",
        "        b_x = x.to(device)  # 將輸入數據移動到GPU上\n",
        "        b_y = y.to(device)  # 將標籤移動到GPU上\n",
        "        out = Resnet_20(b_x)\n",
        "        loss = loss_function(out, b_y)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if step % 100 == 0:\n",
        "            print('Epoch: {} | Step: {} | Loss: {}'.format(epoch + 1, step, loss))\n",
        "\n",
        "\n",
        "# 容器來儲存每一輪的輸出\n",
        "train_predict = torch.Tensor().to(device)\n",
        "train_label = torch.Tensor().to(device)\n",
        "test_predict = torch.Tensor().to(device)\n",
        "test_label = torch.Tensor().to(device)\n",
        "\n",
        "# 在評估模式下，模型只需要在主 GPU 上執行\n",
        "Resnet_20 = Resnet_20.module\n",
        "\n",
        "local_train_loader = copy.deepcopy(train_loader)\n",
        "local_test_loader = copy.deepcopy(test_loader)\n",
        "# 測試集\n",
        "Resnet_20.eval()  # 切換到評估模式\n",
        "model_path = '/kaggle/working/'\n",
        "torch.save(Resnet_20.state_dict(), model_path+'Resnet_20.pth')\n",
        "\n",
        "\n",
        "with torch.no_grad():  # 在評估模式下，不計算梯度\n",
        "    for step, (x, y) in enumerate(local_train_loader):\n",
        "        b_x = x.to(device)\n",
        "        b_y = y.to(device)\n",
        "        out = Resnet_20(b_x)\n",
        "        # 將該批次的預測結果和標籤添加到對應的容器中\n",
        "        train_predict = torch.cat((train_predict, out), dim=0)\n",
        "        train_label = torch.cat((train_label, b_y), dim=0)\n",
        "\n",
        "with torch.no_grad():  # 在評估模式下，不計算梯度\n",
        "    for step, (x, y) in enumerate(local_test_loader):\n",
        "        b_x = x.to(device)\n",
        "        b_y = y.to(device)\n",
        "        out = Resnet_20(b_x)\n",
        "        # 將該批次的預測結果和標籤添加到對應的容器中\n",
        "        test_predict = torch.cat((test_predict, out), dim=0)\n",
        "        test_label = torch.cat((test_label, b_y), dim=0)\n",
        "\n",
        "\n",
        "# train_predict、train_label 分別包含了每一輪的模型訓練集預測和標籤\n",
        "# test_predict、test_label 分別包含了模型對測試集的預測和標籤\n",
        "print(\"Shape of train_predict:\", train_predict.shape)\n",
        "print(\"Shape of train_label:\", train_label.shape)\n",
        "print(\"Shape of test_predict:\", test_predict.shape)\n",
        "print(\"Shape of test_label:\", test_label.shape)\n",
        "\n",
        "\n",
        "output_path = OUTPUT_PATH+'train'\n",
        "if not os.path.exists(output_path):\n",
        "    os.mkdir(output_path)\n",
        "temp_dict = {'predictionVectors':train_predict,'labelVectors':train_label}\n",
        "torch.save(temp_dict, os.path.join(output_path, 'Resnet_20.pt'))\n",
        "\n",
        "\n",
        "output_path = OUTPUT_PATH+'test'\n",
        "if not os.path.exists(output_path):\n",
        "    os.mkdir(output_path)\n",
        "temp_dict = {'predictionVectors':test_predict,'labelVectors':test_label}\n",
        "torch.save(temp_dict, os.path.join(output_path, 'Resnet_20.pt'))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-10T01:39:54.320929Z",
          "iopub.execute_input": "2023-12-10T01:39:54.321256Z",
          "iopub.status.idle": "2023-12-10T01:39:54.335505Z",
          "shell.execute_reply.started": "2023-12-10T01:39:54.321225Z",
          "shell.execute_reply": "2023-12-10T01:39:54.334663Z"
        },
        "trusted": true,
        "id": "ZG8LwHPbH9Ki"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "Resnet_32 = resnet(depth=32).cuda()\n",
        "# 使用 DataParallel 將模型複製到多個 GPU\n",
        "Resnet_32 = nn.DataParallel(Resnet_32)\n",
        "\n",
        "optimizer = torch.optim.Adam(Resnet_32.parameters(), lr=LR)\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "local_train_data = copy.deepcopy(train_loader)\n",
        "\n",
        "for epoch in range(EPOCH):\n",
        "    Resnet_32.train()\n",
        "    for step, (x, y) in enumerate(local_train_data):\n",
        "        # b_x = Variable(x, requires_grad=False)\n",
        "        # b_y = Variable(y, requires_grad=False)\n",
        "        b_x = x.to(device)  # 將輸入數據移動到GPU上\n",
        "        b_y = y.to(device)  # 將標籤移動到GPU上\n",
        "        out = Resnet_32(b_x)\n",
        "        loss = loss_function(out, b_y)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if step % 100 == 0:\n",
        "            print('Epoch: {} | Step: {} | Loss: {}'.format(epoch + 1, step, loss))\n",
        "\n",
        "\n",
        "# 容器來儲存每一輪的輸出\n",
        "train_predict = torch.Tensor().to(device)\n",
        "train_label = torch.Tensor().to(device)\n",
        "test_predict = torch.Tensor().to(device)\n",
        "test_label = torch.Tensor().to(device)\n",
        "\n",
        "# 在評估模式下，模型只需要在主 GPU 上執行\n",
        "Resnet_32 = Resnet_32.module\n",
        "\n",
        "local_train_loader = copy.deepcopy(train_loader)\n",
        "local_test_loader = copy.deepcopy(test_loader)\n",
        "# 測試集\n",
        "Resnet_32.eval()  # 切換到評估模式\n",
        "model_path = '/kaggle/working/'\n",
        "torch.save(Resnet_32.state_dict(), model_path+'Resnet_32.pth')\n",
        "\n",
        "\n",
        "with torch.no_grad():  # 在評估模式下，不計算梯度\n",
        "    for step, (x, y) in enumerate(local_train_loader):\n",
        "        b_x = x.to(device)\n",
        "        b_y = y.to(device)\n",
        "        out = Resnet_32(b_x)\n",
        "        # 將該批次的預測結果和標籤添加到對應的容器中\n",
        "        train_predict = torch.cat((train_predict, out), dim=0)\n",
        "        train_label = torch.cat((train_label, b_y), dim=0)\n",
        "\n",
        "with torch.no_grad():  # 在評估模式下，不計算梯度\n",
        "    for step, (x, y) in enumerate(local_test_loader):\n",
        "        b_x = x.to(device)\n",
        "        b_y = y.to(device)\n",
        "        out = Resnet_32(b_x)\n",
        "        # 將該批次的預測結果和標籤添加到對應的容器中\n",
        "        test_predict = torch.cat((test_predict, out), dim=0)\n",
        "        test_label = torch.cat((test_label, b_y), dim=0)\n",
        "\n",
        "# train_predict、train_label 分別包含了每一輪的模型訓練集預測和標籤\n",
        "# test_predict、test_label 分別包含了模型對測試集的預測和標籤\n",
        "print(\"Shape of train_predict:\", train_predict.shape)\n",
        "print(\"Shape of train_label:\", train_label.shape)\n",
        "print(\"Shape of test_predict:\", test_predict.shape)\n",
        "print(\"Shape of test_label:\", test_label.shape)\n",
        "\n",
        "\n",
        "output_path = OUTPUT_PATH+'train'\n",
        "if not os.path.exists(output_path):\n",
        "    os.mkdir(output_path)\n",
        "temp_dict = {'predictionVectors':train_predict,'labelVectors':train_label}\n",
        "torch.save(temp_dict, os.path.join(output_path, 'Resnet_32.pt'))\n",
        "\n",
        "\n",
        "output_path = OUTPUT_PATH+'test'\n",
        "if not os.path.exists(output_path):\n",
        "    os.mkdir(output_path)\n",
        "temp_dict = {'predictionVectors':test_predict,'labelVectors':test_label}\n",
        "torch.save(temp_dict, os.path.join(output_path, 'Resnet_32.pt'))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-10T01:39:54.336903Z",
          "iopub.execute_input": "2023-12-10T01:39:54.337163Z",
          "iopub.status.idle": "2023-12-10T01:39:54.350967Z",
          "shell.execute_reply.started": "2023-12-10T01:39:54.337140Z",
          "shell.execute_reply": "2023-12-10T01:39:54.350119Z"
        },
        "trusted": true,
        "id": "pcJTJ6hkH9Ki"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "Resnet_44 = resnet(depth=44).cuda()\n",
        "# 使用 DataParallel 將模型複製到多個 GPU\n",
        "Resnet_44 = nn.DataParallel(Resnet_44)\n",
        "\n",
        "optimizer = torch.optim.Adam(Resnet_44.parameters(), lr=LR)\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "local_train_data = copy.deepcopy(train_loader)\n",
        "\n",
        "for epoch in range(EPOCH):\n",
        "    Resnet_44.train()\n",
        "    for step, (x, y) in enumerate(local_train_data):\n",
        "        # b_x = Variable(x, requires_grad=False)\n",
        "        # b_y = Variable(y, requires_grad=False)\n",
        "        b_x = x.to(device)  # 將輸入數據移動到GPU上\n",
        "        b_y = y.to(device)  # 將標籤移動到GPU上\n",
        "        out = Resnet_44(b_x)\n",
        "        loss = loss_function(out, b_y)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if step % 100 == 0:\n",
        "            print('Epoch: {} | Step: {} | Loss: {}'.format(epoch + 1, step, loss))\n",
        "\n",
        "\n",
        "# 容器來儲存每一輪的輸出\n",
        "train_predict = torch.Tensor().to(device)\n",
        "train_label = torch.Tensor().to(device)\n",
        "test_predict = torch.Tensor().to(device)\n",
        "test_label = torch.Tensor().to(device)\n",
        "\n",
        "# 在評估模式下，模型只需要在主 GPU 上執行\n",
        "Resnet_44 = Resnet_44.module\n",
        "\n",
        "local_train_loader = copy.deepcopy(train_loader)\n",
        "local_test_loader = copy.deepcopy(test_loader)\n",
        "# 測試集\n",
        "Resnet_44.eval()  # 切換到評估模式\n",
        "model_path = '/kaggle/working/'\n",
        "torch.save(Resnet_44.state_dict(), model_path+'Resnet_44.pth')\n",
        "\n",
        "\n",
        "with torch.no_grad():  # 在評估模式下，不計算梯度\n",
        "    for step, (x, y) in enumerate(local_train_loader):\n",
        "        b_x = x.to(device)\n",
        "        b_y = y.to(device)\n",
        "        out = Resnet_44(b_x)\n",
        "        # 將該批次的預測結果和標籤添加到對應的容器中\n",
        "        train_predict = torch.cat((train_predict, out), dim=0)\n",
        "        train_label = torch.cat((train_label, b_y), dim=0)\n",
        "\n",
        "with torch.no_grad():  # 在評估模式下，不計算梯度\n",
        "    for step, (x, y) in enumerate(local_test_loader):\n",
        "        b_x = x.to(device)\n",
        "        b_y = y.to(device)\n",
        "        out = Resnet_44(b_x)\n",
        "        # 將該批次的預測結果和標籤添加到對應的容器中\n",
        "        test_predict = torch.cat((test_predict, out), dim=0)\n",
        "        test_label = torch.cat((test_label, b_y), dim=0)\n",
        "\n",
        "# train_predict、train_label 分別包含了每一輪的模型訓練集預測和標籤\n",
        "# test_predict、test_label 分別包含了模型對測試集的預測和標籤\n",
        "print(\"Shape of train_predict:\", train_predict.shape)\n",
        "print(\"Shape of train_label:\", train_label.shape)\n",
        "print(\"Shape of test_predict:\", test_predict.shape)\n",
        "print(\"Shape of test_label:\", test_label.shape)\n",
        "\n",
        "\n",
        "output_path = OUTPUT_PATH+'train'\n",
        "if not os.path.exists(output_path):\n",
        "    os.mkdir(output_path)\n",
        "temp_dict = {'predictionVectors':train_predict,'labelVectors':train_label}\n",
        "torch.save(temp_dict, os.path.join(output_path, 'Resnet_44.pt'))\n",
        "\n",
        "\n",
        "output_path = OUTPUT_PATH+'test'\n",
        "if not os.path.exists(output_path):\n",
        "    os.mkdir(output_path)\n",
        "temp_dict = {'predictionVectors':test_predict,'labelVectors':test_label}\n",
        "torch.save(temp_dict, os.path.join(output_path, 'Resnet_44.pt'))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-10T01:39:54.352128Z",
          "iopub.execute_input": "2023-12-10T01:39:54.352406Z",
          "iopub.status.idle": "2023-12-10T01:39:54.368332Z",
          "shell.execute_reply.started": "2023-12-10T01:39:54.352378Z",
          "shell.execute_reply": "2023-12-10T01:39:54.367471Z"
        },
        "trusted": true,
        "id": "yZceaCTQH9Kj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "Resnet_56 = resnet(depth=56).cuda()\n",
        "# 使用 DataParallel 將模型複製到多個 GPU\n",
        "Resnet_56 = nn.DataParallel(Resnet_56)\n",
        "\n",
        "optimizer = torch.optim.Adam(Resnet_56.parameters(), lr=LR)\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "local_train_data = copy.deepcopy(train_loader)\n",
        "\n",
        "for epoch in range(EPOCH):\n",
        "    Resnet_56.train()\n",
        "    for step, (x, y) in enumerate(local_train_data):\n",
        "        # b_x = Variable(x, requires_grad=False)\n",
        "        # b_y = Variable(y, requires_grad=False)\n",
        "        b_x = x.to(device)  # 將輸入數據移動到GPU上\n",
        "        b_y = y.to(device)  # 將標籤移動到GPU上\n",
        "        out = Resnet_56(b_x)\n",
        "        loss = loss_function(out, b_y)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if step % 100 == 0:\n",
        "            print('Epoch: {} | Step: {} | Loss: {}'.format(epoch + 1, step, loss))\n",
        "\n",
        "\n",
        "# 容器來儲存每一輪的輸出\n",
        "train_predict = torch.Tensor().to(device)\n",
        "train_label = torch.Tensor().to(device)\n",
        "test_predict = torch.Tensor().to(device)\n",
        "test_label = torch.Tensor().to(device)\n",
        "\n",
        "# 在評估模式下，模型只需要在主 GPU 上執行\n",
        "Resnet_56 = Resnet_56.module\n",
        "\n",
        "local_train_loader = copy.deepcopy(train_loader)\n",
        "local_test_loader = copy.deepcopy(test_loader)\n",
        "# 測試集\n",
        "Resnet_56.eval()  # 切換到評估模式\n",
        "model_path = '/kaggle/working/'\n",
        "torch.save(Resnet_56.state_dict(), model_path+'Resnet_56.pth')\n",
        "\n",
        "\n",
        "with torch.no_grad():  # 在評估模式下，不計算梯度\n",
        "    for step, (x, y) in enumerate(local_train_loader):\n",
        "        b_x = x.to(device)\n",
        "        b_y = y.to(device)\n",
        "        out = Resnet_56(b_x)\n",
        "        # 將該批次的預測結果和標籤添加到對應的容器中\n",
        "        train_predict = torch.cat((train_predict, out), dim=0)\n",
        "        train_label = torch.cat((train_label, b_y), dim=0)\n",
        "\n",
        "with torch.no_grad():  # 在評估模式下，不計算梯度\n",
        "    for step, (x, y) in enumerate(local_test_loader):\n",
        "        b_x = x.to(device)\n",
        "        b_y = y.to(device)\n",
        "        out = Resnet_56(b_x)\n",
        "        # 將該批次的預測結果和標籤添加到對應的容器中\n",
        "        test_predict = torch.cat((test_predict, out), dim=0)\n",
        "        test_label = torch.cat((test_label, b_y), dim=0)\n",
        "\n",
        "# train_predict、train_label 分別包含了每一輪的模型訓練集預測和標籤\n",
        "# test_predict、test_label 分別包含了模型對測試集的預測和標籤\n",
        "print(\"Shape of train_predict:\", train_predict.shape)\n",
        "print(\"Shape of train_label:\", train_label.shape)\n",
        "print(\"Shape of test_predict:\", test_predict.shape)\n",
        "print(\"Shape of test_label:\", test_label.shape)\n",
        "\n",
        "\n",
        "output_path = OUTPUT_PATH+'train'\n",
        "if not os.path.exists(output_path):\n",
        "    os.mkdir(output_path)\n",
        "temp_dict = {'predictionVectors':train_predict,'labelVectors':train_label}\n",
        "torch.save(temp_dict, os.path.join(output_path, 'Resnet_56.pt'))\n",
        "\n",
        "\n",
        "output_path = OUTPUT_PATH+'test'\n",
        "if not os.path.exists(output_path):\n",
        "    os.mkdir(output_path)\n",
        "temp_dict = {'predictionVectors':test_predict,'labelVectors':test_label}\n",
        "torch.save(temp_dict, os.path.join(output_path, 'Resnet_56.pt'))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-10T01:39:54.369560Z",
          "iopub.execute_input": "2023-12-10T01:39:54.369850Z",
          "iopub.status.idle": "2023-12-10T01:39:54.383532Z",
          "shell.execute_reply.started": "2023-12-10T01:39:54.369816Z",
          "shell.execute_reply": "2023-12-10T01:39:54.382674Z"
        },
        "trusted": true,
        "id": "GW9WLN_dH9Kj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "Resnet_110 = resnet(depth=110).cuda()\n",
        "# 使用 DataParallel 將模型複製到多個 GPU\n",
        "Resnet_110 = nn.DataParallel(Resnet_110)\n",
        "\n",
        "optimizer = torch.optim.Adam(Resnet_110.parameters(), lr=LR)\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "local_train_data = copy.deepcopy(train_loader)\n",
        "\n",
        "for epoch in range(EPOCH):\n",
        "    Resnet_110.train()\n",
        "    for step, (x, y) in enumerate(local_train_data):\n",
        "        # b_x = Variable(x, requires_grad=False)\n",
        "        # b_y = Variable(y, requires_grad=False)\n",
        "        b_x = x.to(device)  # 將輸入數據移動到GPU上\n",
        "        b_y = y.to(device)  # 將標籤移動到GPU上\n",
        "        out = Resnet_110(b_x)\n",
        "        loss = loss_function(out, b_y)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if step % 100 == 0:\n",
        "            print('Epoch: {} | Step: {} | Loss: {}'.format(epoch + 1, step, loss))\n",
        "\n",
        "\n",
        "# 容器來儲存每一輪的輸出\n",
        "train_predict = torch.Tensor().to(device)\n",
        "train_label = torch.Tensor().to(device)\n",
        "test_predict = torch.Tensor().to(device)\n",
        "test_label = torch.Tensor().to(device)\n",
        "\n",
        "# 在評估模式下，模型只需要在主 GPU 上執行\n",
        "Resnet_110 = Resnet_110.module\n",
        "\n",
        "local_train_loader = copy.deepcopy(train_loader)\n",
        "local_test_loader = copy.deepcopy(test_loader)\n",
        "# 測試集\n",
        "Resnet_110.eval()  # 切換到評估模式\n",
        "model_path = '/kaggle/working/'\n",
        "torch.save(Resnet_110.state_dict(), model_path+'Resnet_110.pth')\n",
        "\n",
        "\n",
        "with torch.no_grad():  # 在評估模式下，不計算梯度\n",
        "    for step, (x, y) in enumerate(local_train_loader):\n",
        "        b_x = x.to(device)\n",
        "        b_y = y.to(device)\n",
        "        out = Resnet_110(b_x)\n",
        "        # 將該批次的預測結果和標籤添加到對應的容器中\n",
        "        train_predict = torch.cat((train_predict, out), dim=0)\n",
        "        train_label = torch.cat((train_label, b_y), dim=0)\n",
        "\n",
        "with torch.no_grad():  # 在評估模式下，不計算梯度\n",
        "    for step, (x, y) in enumerate(local_test_loader):\n",
        "        b_x = x.to(device)\n",
        "        b_y = y.to(device)\n",
        "        out = Resnet_110(b_x)\n",
        "        # 將該批次的預測結果和標籤添加到對應的容器中\n",
        "        test_predict = torch.cat((test_predict, out), dim=0)\n",
        "        test_label = torch.cat((test_label, b_y), dim=0)\n",
        "\n",
        "# train_predict、train_label 分別包含了每一輪的模型訓練集預測和標籤\n",
        "# test_predict、test_label 分別包含了模型對測試集的預測和標籤\n",
        "print(\"Shape of train_predict:\", train_predict.shape)\n",
        "print(\"Shape of train_label:\", train_label.shape)\n",
        "print(\"Shape of test_predict:\", test_predict.shape)\n",
        "print(\"Shape of test_label:\", test_label.shape)\n",
        "\n",
        "\n",
        "output_path = OUTPUT_PATH+'train'\n",
        "if not os.path.exists(output_path):\n",
        "    os.mkdir(output_path)\n",
        "temp_dict = {'predictionVectors':train_predict,'labelVectors':train_label}\n",
        "torch.save(temp_dict, os.path.join(output_path, 'Resnet_110_train_prediction.pt'))\n",
        "\n",
        "\n",
        "output_path = OUTPUT_PATH+'test'\n",
        "if not os.path.exists(output_path):\n",
        "    os.mkdir(output_path)\n",
        "temp_dict = {'predictionVectors':test_predict,'labelVectors':test_label}\n",
        "torch.save(temp_dict, os.path.join(output_path, 'Resnet_110_test_prediction.pt'))"
      ],
      "metadata": {
        "id": "mZv3ST309E2R",
        "execution": {
          "iopub.status.busy": "2023-12-10T01:39:54.384603Z",
          "iopub.execute_input": "2023-12-10T01:39:54.384890Z",
          "iopub.status.idle": "2023-12-10T04:29:58.131026Z",
          "shell.execute_reply.started": "2023-12-10T01:39:54.384866Z",
          "shell.execute_reply": "2023-12-10T04:29:58.129939Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# VGG"
      ],
      "metadata": {
        "id": "yDbB9MoYH9Kj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''VGG for CIFAR10. FC layers are removed.\n",
        "'''\n",
        "import torch.nn as nn\n",
        "import torch.utils.model_zoo as model_zoo\n",
        "import math\n",
        "\n",
        "\n",
        "__all__ = [\n",
        "    'VGG', 'vgg11', 'vgg11_bn', 'vgg13', 'vgg13_bn', 'vgg16', 'vgg16_bn',\n",
        "    'vgg19_bn', 'vgg19',\n",
        "]\n",
        "\n",
        "\n",
        "model_urls = {\n",
        "    'vgg11': 'https://download.pytorch.org/models/vgg11-bbd30ac9.pth',\n",
        "    'vgg13': 'https://download.pytorch.org/models/vgg13-c768596a.pth',\n",
        "    'vgg16': 'https://download.pytorch.org/models/vgg16-397923af.pth',\n",
        "    'vgg19': 'https://download.pytorch.org/models/vgg19-dcbb9e9d.pth',\n",
        "}\n",
        "\n",
        "\n",
        "class VGG(nn.Module):\n",
        "\n",
        "    def __init__(self, features, num_classes=45):\n",
        "        super(VGG, self).__init__()\n",
        "        self.features = features\n",
        "        self.classifier = nn.Linear(512*64, num_classes)\n",
        "        self._initialize_weights()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
        "                if m.bias is not None:\n",
        "                    m.bias.data.zero_()\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                m.weight.data.fill_(1)\n",
        "                m.bias.data.zero_()\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                n = m.weight.size(1)\n",
        "                m.weight.data.normal_(0, 0.01)\n",
        "                m.bias.data.zero_()\n",
        "\n",
        "\n",
        "def make_layers(cfg, batch_norm=False):\n",
        "    layers = []\n",
        "    in_channels = 3\n",
        "    for v in cfg:\n",
        "        if v == 'M':\n",
        "            layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
        "        else:\n",
        "            conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1)\n",
        "            if batch_norm:\n",
        "                layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)]\n",
        "            else:\n",
        "                layers += [conv2d, nn.ReLU(inplace=True)]\n",
        "            in_channels = v\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "\n",
        "cfg = {\n",
        "    'A': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
        "    'B': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
        "    'D': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
        "    'E': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n",
        "}\n",
        "\n",
        "\n",
        "def vgg11(**kwargs):\n",
        "    '''VGG 11-layer model (configuration \"A\")\n",
        "\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "    '''\n",
        "    model = VGG(make_layers(cfg['A']), **kwargs)\n",
        "    return model\n",
        "\n",
        "\n",
        "def vgg11_bn(**kwargs):\n",
        "    '''VGG 11-layer model (configuration \"A\") with batch normalization'''\n",
        "    model = VGG(make_layers(cfg['A'], batch_norm=True), **kwargs)\n",
        "    return model\n",
        "\n",
        "\n",
        "def vgg13(**kwargs):\n",
        "    '''VGG 13-layer model (configuration \"B\")\n",
        "\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "    '''\n",
        "    model = VGG(make_layers(cfg['B']), **kwargs)\n",
        "    return model\n",
        "\n",
        "\n",
        "def vgg13_bn(**kwargs):\n",
        "    '''VGG 13-layer model (configuration \"B\") with batch normalization'''\n",
        "    model = VGG(make_layers(cfg['B'], batch_norm=True), **kwargs)\n",
        "    return model\n",
        "\n",
        "\n",
        "def vgg16(**kwargs):\n",
        "    '''VGG 16-layer model (configuration \"D\")\n",
        "\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "    '''\n",
        "    model = VGG(make_layers(cfg['D']), **kwargs)\n",
        "    return model\n",
        "\n",
        "\n",
        "def vgg16_bn(**kwargs):\n",
        "    '''VGG 16-layer model (configuration \"D\") with batch normalization'''\n",
        "    model = VGG(make_layers(cfg['D'], batch_norm=True), **kwargs)\n",
        "    return model\n",
        "\n",
        "\n",
        "def vgg19(**kwargs):\n",
        "    '''VGG 19-layer model (configuration \"E\")\n",
        "\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "    '''\n",
        "    model = VGG(make_layers(cfg['E']), **kwargs)\n",
        "    return model\n",
        "\n",
        "\n",
        "def vgg19_bn(**kwargs):\n",
        "    '''VGG 19-layer model (configuration 'E') with batch normalization'''\n",
        "    model = VGG(make_layers(cfg['E'], batch_norm=True), **kwargs)\n",
        "    return model"
      ],
      "metadata": {
        "id": "oimdShWQH9Kj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "Vgg16_bn = vgg16_bn().cuda()\n",
        "\n",
        "Vgg16_bn = nn.DataParallel(Vgg16_bn)\n",
        "\n",
        "optimizer = torch.optim.Adam(Vgg16_bn.parameters(), lr=LR)\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "local_train_data = copy.deepcopy(train_loader)\n",
        "\n",
        "for epoch in range(EPOCH):\n",
        "    Vgg16_bn.train()\n",
        "    for step, (x, y) in enumerate(local_train_data):\n",
        "        # b_x = Variable(x, requires_grad=False)\n",
        "        # b_y = Variable(y, requires_grad=False)\n",
        "        b_x = x.to(device)  # 將輸入數據移動到GPU上\n",
        "        b_y = y.to(device)  # 將標籤移動到GPU上\n",
        "        out = Vgg16_bn(b_x)\n",
        "        loss = loss_function(out, b_y)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if step % 100 == 0:\n",
        "            print('Epoch: {} | Step: {} | Loss: {}'.format(epoch + 1, step, loss))\n",
        "\n",
        "# 容器來儲存每一輪的輸出\n",
        "train_predict = torch.Tensor().to(device)\n",
        "train_label = torch.Tensor().to(device)\n",
        "test_predict = torch.Tensor().to(device)\n",
        "test_label = torch.Tensor().to(device)\n",
        "\n",
        "\n",
        "# 在評估模式下，模型只需要在主 GPU 上執行\n",
        "Vgg16_bn = Vgg16_bn.module\n",
        "\n",
        "local_train_loader = copy.deepcopy(train_loader)\n",
        "local_test_loader = copy.deepcopy(test_loader)\n",
        "# 測試集\n",
        "Vgg16_bn.eval()  # 切換到評估模式\n",
        "model_path = '/kaggle/working/'\n",
        "torch.save(Vgg16_bn.state_dict(), model_path+'Vgg16_bn.pth')\n",
        "\n",
        "\n",
        "with torch.no_grad():  # 在評估模式下，不計算梯度\n",
        "    for step, (x, y) in enumerate(local_train_loader):\n",
        "        b_x = x.to(device)\n",
        "        b_y = y.to(device)\n",
        "        out = Vgg16_bn(b_x)\n",
        "        # 將該批次的預測結果和標籤添加到對應的容器中\n",
        "        train_predict = torch.cat((train_predict, out), dim=0)\n",
        "        train_label = torch.cat((train_label, b_y), dim=0)\n",
        "\n",
        "with torch.no_grad():  # 在評估模式下，不計算梯度\n",
        "    for step, (x, y) in enumerate(local_test_loader):\n",
        "        b_x = x.to(device)\n",
        "        b_y = y.to(device)\n",
        "        out = Vgg16_bn(b_x)\n",
        "        # 將該批次的預測結果和標籤添加到對應的容器中\n",
        "        test_predict = torch.cat((test_predict, out), dim=0)\n",
        "        test_label = torch.cat((test_label, b_y), dim=0)\n",
        "\n",
        "# train_predict、train_label 分別包含了每一輪的模型訓練集預測和標籤\n",
        "# test_predict、test_label 分別包含了模型對測試集的預測和標籤\n",
        "print(\"Shape of train_predict:\", train_predict.shape)\n",
        "print(\"Shape of train_label:\", train_label.shape)\n",
        "print(\"Shape of test_predict:\", test_predict.shape)\n",
        "print(\"Shape of test_label:\", test_label.shape)\n",
        "\n",
        "\n",
        "output_path = OUTPUT_PATH+'train'\n",
        "if not os.path.exists(output_path):\n",
        "    os.mkdir(output_path)\n",
        "temp_dict = {'predictionVectors':train_predict,'labelVectors':train_label}\n",
        "torch.save(temp_dict, os.path.join(output_path, 'Vgg16_bn_train_prediction.pt'))\n",
        "\n",
        "\n",
        "output_path = OUTPUT_PATH+'test'\n",
        "if not os.path.exists(output_path):\n",
        "    os.mkdir(output_path)\n",
        "temp_dict = {'predictionVectors':test_predict,'labelVectors':test_label}\n",
        "torch.save(temp_dict, os.path.join(output_path, 'Vgg16_bn_test_prediction.pt'))"
      ],
      "metadata": {
        "id": "mZ6d1fcCH9Kk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "Vgg19_bn = vgg19_bn().cuda()\n",
        "\n",
        "Vgg19_bn = nn.DataParallel(Vgg19_bn)\n",
        "optimizer = torch.optim.Adam(Vgg19_bn.parameters(), lr=LR)\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "local_train_data = copy.deepcopy(train_loader)\n",
        "\n",
        "for epoch in range(EPOCH):\n",
        "    Vgg19_bn.train()\n",
        "    for step, (x, y) in enumerate(local_train_data):\n",
        "        # b_x = Variable(x, requires_grad=False)\n",
        "        # b_y = Variable(y, requires_grad=False)\n",
        "        b_x = x.to(device)  # 將輸入數據移動到GPU上\n",
        "        b_y = y.to(device)  # 將標籤移動到GPU上\n",
        "        out = Vgg19_bn(b_x)\n",
        "        loss = loss_function(out, b_y)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if step % 100 == 0:\n",
        "            print('Epoch: {} | Step: {} | Loss: {}'.format(epoch + 1, step, loss))\n",
        "\n",
        "\n",
        "# 容器來儲存每一輪的輸出\n",
        "train_predict = torch.Tensor().to(device)\n",
        "train_label = torch.Tensor().to(device)\n",
        "test_predict = torch.Tensor().to(device)\n",
        "test_label = torch.Tensor().to(device)\n",
        "\n",
        "\n",
        "# 在評估模式下，模型只需要在主 GPU 上執行\n",
        "Vgg19_bn = Vgg19_bn.module\n",
        "\n",
        "local_train_loader = copy.deepcopy(train_loader)\n",
        "local_test_loader = copy.deepcopy(test_loader)\n",
        "# 測試集\n",
        "Vgg19_bn.eval()  # 切換到評估模式\n",
        "model_path = '/kaggle/working/'\n",
        "torch.save(Vgg19_bn.state_dict(), model_path+'Vgg19_bn.pth')\n",
        "\n",
        "\n",
        "with torch.no_grad():  # 在評估模式下，不計算梯度\n",
        "    for step, (x, y) in enumerate(local_train_loader):\n",
        "        b_x = x.to(device)\n",
        "        b_y = y.to(device)\n",
        "        out = Vgg19_bn(b_x)\n",
        "        # 將該批次的預測結果和標籤添加到對應的容器中\n",
        "        train_predict = torch.cat((train_predict, out), dim=0)\n",
        "        train_label = torch.cat((train_label, b_y), dim=0)\n",
        "\n",
        "with torch.no_grad():  # 在評估模式下，不計算梯度\n",
        "    for step, (x, y) in enumerate(local_test_loader):\n",
        "        b_x = x.to(device)\n",
        "        b_y = y.to(device)\n",
        "        out = Vgg19_bn(b_x)\n",
        "        # 將該批次的預測結果和標籤添加到對應的容器中\n",
        "        test_predict = torch.cat((test_predict, out), dim=0)\n",
        "        test_label = torch.cat((test_label, b_y), dim=0)\n",
        "\n",
        "# train_predict、train_label 分別包含了每一輪的模型訓練集預測和標籤\n",
        "# test_predict、test_label 分別包含了模型對測試集的預測和標籤\n",
        "print(\"Shape of train_predict:\", train_predict.shape)\n",
        "print(\"Shape of train_label:\", train_label.shape)\n",
        "print(\"Shape of test_predict:\", test_predict.shape)\n",
        "print(\"Shape of test_label:\", test_label.shape)\n",
        "\n",
        "\n",
        "output_path = OUTPUT_PATH+'train'\n",
        "if not os.path.exists(output_path):\n",
        "    os.mkdir(output_path)\n",
        "temp_dict = {'predictionVectors':train_predict,'labelVectors':train_label}\n",
        "torch.save(temp_dict, os.path.join(output_path, 'Vgg19_bn_train_prediction.pt'))\n",
        "\n",
        "\n",
        "output_path = OUTPUT_PATH+'test'\n",
        "if not os.path.exists(output_path):\n",
        "    os.mkdir(output_path)\n",
        "temp_dict = {'predictionVectors':test_predict,'labelVectors':test_label}\n",
        "torch.save(temp_dict, os.path.join(output_path, 'Vgg19_bn_test_prediction.pt'))"
      ],
      "metadata": {
        "id": "zSp7OM-2H9Kk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Wrn\n"
      ],
      "metadata": {
        "id": "cu4xNlr2H9Kk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    def __init__(self, in_planes, out_planes, stride, dropRate=0.0):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.bn1 = nn.BatchNorm2d(in_planes)\n",
        "        self.relu1 = nn.ReLU(inplace=True)\n",
        "        self.conv1 = nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
        "                               padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_planes)\n",
        "        self.relu2 = nn.ReLU(inplace=True)\n",
        "        self.conv2 = nn.Conv2d(out_planes, out_planes, kernel_size=3, stride=1,\n",
        "                               padding=1, bias=False)\n",
        "        self.droprate = dropRate\n",
        "        self.equalInOut = (in_planes == out_planes)\n",
        "        self.convShortcut = (not self.equalInOut) and nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride,\n",
        "                               padding=0, bias=False) or None\n",
        "    def forward(self, x):\n",
        "        if not self.equalInOut:\n",
        "            x = self.relu1(self.bn1(x))\n",
        "        else:\n",
        "            out = self.relu1(self.bn1(x))\n",
        "        out = self.relu2(self.bn2(self.conv1(out if self.equalInOut else x)))\n",
        "        if self.droprate > 0:\n",
        "            out = F.dropout(out, p=self.droprate, training=self.training)\n",
        "        out = self.conv2(out)\n",
        "        return torch.add(x if self.equalInOut else self.convShortcut(x), out)\n",
        "\n",
        "class NetworkBlock(nn.Module):\n",
        "    def __init__(self, nb_layers, in_planes, out_planes, block, stride, dropRate=0.0):\n",
        "        super(NetworkBlock, self).__init__()\n",
        "        self.layer = self._make_layer(block, in_planes, out_planes, nb_layers, stride, dropRate)\n",
        "    def _make_layer(self, block, in_planes, out_planes, nb_layers, stride, dropRate):\n",
        "        layers = []\n",
        "        for i in range(nb_layers):\n",
        "            layers.append(block(i == 0 and in_planes or out_planes, out_planes, i == 0 and stride or 1, dropRate))\n",
        "        return nn.Sequential(*layers)\n",
        "    def forward(self, x):\n",
        "        return self.layer(x)\n",
        "\n",
        "class WideResNet(nn.Module):\n",
        "    def __init__(self, depth, num_classes, widen_factor=1, dropRate=0.0):\n",
        "        super(WideResNet, self).__init__()\n",
        "        nChannels = [16, 16*widen_factor, 32*widen_factor, 64*widen_factor]\n",
        "        assert (depth - 4) % 6 == 0, 'depth should be 6n+4'\n",
        "        n = (depth - 4) // 6\n",
        "        block = BasicBlock\n",
        "        # 1st conv before any network block\n",
        "        self.conv1 = nn.Conv2d(3, nChannels[0], kernel_size=3, stride=1,\n",
        "                               padding=1, bias=False)\n",
        "        # 1st block\n",
        "        self.block1 = NetworkBlock(n, nChannels[0], nChannels[1], block, 1, dropRate)\n",
        "        # 2nd block\n",
        "        self.block2 = NetworkBlock(n, nChannels[1], nChannels[2], block, 2, dropRate)\n",
        "        # 3rd block\n",
        "        self.block3 = NetworkBlock(n, nChannels[2], nChannels[3], block, 2, dropRate)\n",
        "        # global average pooling and classifier\n",
        "        self.bn1 = nn.BatchNorm2d(nChannels[3])\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.fc = nn.Linear(nChannels[3]*64, num_classes)\n",
        "        self.nChannels = nChannels[3]\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                m.weight.data.fill_(1)\n",
        "                m.bias.data.zero_()\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                m.bias.data.zero_()\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "        out = self.block1(out)\n",
        "        out = self.block2(out)\n",
        "        out = self.block3(out)\n",
        "        out = self.relu(self.bn1(out))\n",
        "        out = F.avg_pool2d(out, 8)\n",
        "        #out = out.view(-1, self.nChannels)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        return self.fc(out)\n",
        "\n",
        "def wrn(**kwargs):\n",
        "    '''\n",
        "    Constructs a Wide Residual Networks.\n",
        "    '''\n",
        "    model = WideResNet(**kwargs)\n",
        "    return model"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-10T04:29:58.133179Z",
          "iopub.execute_input": "2023-12-10T04:29:58.133649Z",
          "iopub.status.idle": "2023-12-10T04:29:58.157240Z",
          "shell.execute_reply.started": "2023-12-10T04:29:58.133610Z",
          "shell.execute_reply": "2023-12-10T04:29:58.156386Z"
        },
        "trusted": true,
        "id": "tICwdpJKH9Kk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "Wrn = wrn(depth=28, num_classes=45).cuda()\n",
        "# 使用 DataParallel 將模型複製到多個 GPU\n",
        "Wrn = nn.DataParallel(Wrn)\n",
        "\n",
        "optimizer = torch.optim.Adam(Wrn.parameters(), lr=LR)\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "local_train_data = copy.deepcopy(train_loader)\n",
        "\n",
        "for epoch in range(EPOCH):\n",
        "    Wrn.train()\n",
        "    for step, (x, y) in enumerate(local_train_data):\n",
        "    # b_x = Variable(x, requires_grad=False)\n",
        "    # b_y = Variable(y, requires_grad=False)\n",
        "        b_x = x.to(device)  # 將輸入數據移動到GPU上\n",
        "        b_y = y.to(device)  # 將標籤移動到GPU上\n",
        "        out = Wrn(b_x)\n",
        "        loss = loss_function(out, b_y)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if step % 100 == 0:\n",
        "            print('Epoch: {} | Step: {} | Loss: {}'.format(epoch + 1, step, loss))\n",
        "\n",
        "\n",
        "# 容器來儲存每一輪的輸出\n",
        "train_predict = torch.Tensor().to(device)\n",
        "train_label = torch.Tensor().to(device)\n",
        "test_predict = torch.Tensor().to(device)\n",
        "test_label = torch.Tensor().to(device)\n",
        "\n",
        "# 在評估模式下，模型只需要在主 GPU 上執行\n",
        "Wrn = Wrn.module\n",
        "\n",
        "local_train_loader = copy.deepcopy(train_loader)\n",
        "local_test_loader = copy.deepcopy(test_loader)\n",
        "# 測試集\n",
        "Wrn.eval()  # 切換到評估模式\n",
        "model_path = '/kaggle/working/'\n",
        "torch.save(Wrn.state_dict(), model_path+'Wrn.pth')\n",
        "\n",
        "\n",
        "with torch.no_grad():  # 在評估模式下，不計算梯度\n",
        "    for step, (x, y) in enumerate(local_train_loader):\n",
        "        b_x = x.to(device)\n",
        "        b_y = y.to(device)\n",
        "        out = Wrn(b_x)\n",
        "        # 將該批次的預測結果和標籤添加到對應的容器中\n",
        "        train_predict = torch.cat((train_predict, out), dim=0)\n",
        "        train_label = torch.cat((train_label, b_y), dim=0)\n",
        "\n",
        "with torch.no_grad():  # 在評估模式下，不計算梯度\n",
        "    for step, (x, y) in enumerate(local_test_loader):\n",
        "        b_x = x.to(device)\n",
        "        b_y = y.to(device)\n",
        "        out = Wrn(b_x)\n",
        "        # 將該批次的預測結果和標籤添加到對應的容器中\n",
        "        test_predict = torch.cat((test_predict, out), dim=0)\n",
        "        test_label = torch.cat((test_label, b_y), dim=0)\n",
        "\n",
        "# train_predict、train_label 分別包含了每一輪的模型訓練集預測和標籤\n",
        "# test_predict、test_label 分別包含了模型對測試集的預測和標籤\n",
        "print(\"Shape of train_predict:\", train_predict.shape)\n",
        "print(\"Shape of train_label:\", train_label.shape)\n",
        "print(\"Shape of test_predict:\", test_predict.shape)\n",
        "print(\"Shape of test_label:\", test_label.shape)\n",
        "\n",
        "\n",
        "output_path = OUTPUT_PATH+'train'\n",
        "if not os.path.exists(output_path):\n",
        "    os.mkdir(output_path)\n",
        "temp_dict = {'predictionVectors':train_predict,'labelVectors':train_label}\n",
        "torch.save(temp_dict, os.path.join(output_path, 'Wrn_28_train_prediction.pt'))\n",
        "\n",
        "\n",
        "output_path = OUTPUT_PATH+'test'\n",
        "if not os.path.exists(output_path):\n",
        "    os.mkdir(output_path)\n",
        "temp_dict = {'predictionVectors':test_predict,'labelVectors':test_label}\n",
        "torch.save(temp_dict, os.path.join(output_path, 'Wrn_28_test_prediction.pt'))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-10T04:31:17.236104Z",
          "iopub.execute_input": "2023-12-10T04:31:17.236458Z",
          "iopub.status.idle": "2023-12-10T05:10:32.200019Z",
          "shell.execute_reply.started": "2023-12-10T04:31:17.236430Z",
          "shell.execute_reply": "2023-12-10T05:10:32.198976Z"
        },
        "trusted": true,
        "id": "3t6-vYvcH9Kk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}